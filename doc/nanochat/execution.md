# Execution Module Documentation (`execution.py`)

This module provides a sandboxed environment for executing Python code generated by the LLM. It is adapted from the OpenAI HumanEval execution script.

## Overview

The primary goal is to run code safely by:
- Isolating execution in a separate process.
- Enforcing timeouts.
- Limiting memory usage.
- Disabling dangerous system functions (e.g., file system access, network (partially)).

## Main Function

### `execute_code(code, timeout=5.0, maximum_memory_bytes=256MB) -> ExecutionResult`
Executes the provided python string.

- **Returns**: `ExecutionResult` object containing:
    - `success`: Boolean.
    - `stdout`: Captured standard output.
    - `stderr`: Captured standard error.
    - `error`: Exception message if failed.
    - `timeout`: Boolean indicating if it timed out.

## Safety Mechanisms

`reliability_guard()`:
- Disables `faulthandler`.
- Nullifies built-ins like `exit`, `quit`.
- Nullifies `os` functions: `kill`, `system`, `remove`, `rmdir`, `fork`, etc.
- Nullifies `shutil` functions: `rmtree`, `move`.
- Nullifies `subprocess.Popen`.

## Usage in System

Used by the evaluation harness (not directly by the inference engine currently, as `engine.py` uses a simpler `eval` based calculator tool for the demo). This module is likely intended for more complex code interpretation tasks or benchmarks like HumanEval.
