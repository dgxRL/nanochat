# Nanochat Codebase Overview

This document provides a high-level overview of the `nanochat` library, a lightweight and efficient implementation of GPT-style language models.

## System Architecture

`nanochat` is designed to be modular, efficient, and educational. It includes components for:
- **Model Architecture**: A simplified yet state-of-the-art GPT implementation.
- **Inference Engine**: An efficient generation engine with KV caching and tool use support.
- **Training Utilities**: Distributed optimizers (AdamW, Muon), data loading, and checkpoint management.
- **Evaluation**: Tools for evaluating model performance on standard benchmarks.

## Global Entity Relationship Diagram

The following class diagram illustrates the main classes and their relationships within the `nanochat` package.

```mermaid
classDiagram
    class GPTConfig {
        +int sequence_len
        +int vocab_size
        +int n_layer
        +int n_head
        +int n_kv_head
        +int n_embd
    }

    class GPT {
        +GPTConfig config
        +nn.ModuleDict transformer
        +nn.Linear lm_head
        +forward(idx, targets, kv_cache)
        +generate(tokens, max_tokens)
    }

    class Block {
        +CausalSelfAttention attn
        +MLP mlp
    }

    class CausalSelfAttention {
        +forward(x, cos_sin, kv_cache)
    }

    class MLP {
        +forward(x)
    }

    class Engine {
        +GPT model
        +Tokenizer tokenizer
        +generate(tokens, num_samples, max_tokens)
    }

    class KVCache {
        +insert_kv(layer_idx, k, v)
        +get_pos()
    }

    class Tokenizer {
        <<interface>>
        +encode(text)
        +decode(ids)
    }

    class RustBPETokenizer {
        +train_from_iterator()
    }
    
    class HuggingFaceTokenizer {
        +from_pretrained()
    }

    GPT --> GPTConfig
    GPT *-- Block
    Block *-- CausalSelfAttention
    Block *-- MLP
    Engine --> GPT
    Engine --> Tokenizer
    GPT ..> KVCache : Uses during inference
    Tokenizer <|-- RustBPETokenizer
    Tokenizer <|-- HuggingFaceTokenizer

```

## Inference Control Flow

The following sequence diagram shows the flow of data during text generation using the `Engine`.

```mermaid
sequenceDiagram
    participant User
    participant Engine
    participant GPT
    participant KVCache
    participant Tokenizer

    User->>Engine: generate(tokens)
    Engine->>Tokenizer: encode(tokens) (if str)
    
    rect rgb(240, 248, 255)
    Note over Engine, GPT: Prefill Phase
    Engine->>KVCache: init(batch_size=1)
    Engine->>GPT: forward(ids, kv_cache=prefill)
    GPT->>KVCache: insert_kv(k, v)
    GPT-->>Engine: logits
    end

    rect rgb(255, 250, 240)
    Note over Engine, GPT: Generation Loop
    Engine->>KVCache: init(batch_size=N) (decode cache)
    Engine->>KVCache: prefill(from=prefill_cache)
    
    loop Until max_tokens or EOS
        Engine->>GPT: forward(next_token, kv_cache=decode)
        GPT->>KVCache: insert_kv(k, v)
        GPT-->>Engine: logits
        Engine->>Engine: sample_next_token(logits)
        Engine->>Engine: handle_tools(next_token)
        Engine-->>User: yield generated_tokens
    end
    end
```

## Module Descriptions

- **`gpt.py`**: Defines the `GPT` model, including `CausalSelfAttention` with RoPE and GQA, `MLP`, and `Block`.
- **`engine.py`**: Handles efficient inference, managing `KVCache` and tool execution (e.g., Python calculator).
- **`execution.py`**: safe-guarded execution environment for Python code generated by the model.
- **`tokenizer.py`**: Wrapper for tokenizers, supporting both HuggingFace and custom RustBPE implementations.
- **`dataset.py` / `dataloader.py`**: Handles downloading and streaming of the FineWeb-Edu dataset.
- **`adamw.py` / `muon.py`**: Custom distributed optimizers for training.
- **`common.py` / `checkpoint_manager.py`**: Utilities for distributed training setup, logging, and checkpointing.
- **`core_eval.py` / `loss_eval.py`**: Evaluation metrics and loop.
