Vocab size: 32,768
num_layers: 6
model_dim: 64 (base: 6, nudge: +58)
num_heads: 1
head_dim: 64
num_kv_heads: 1
Tokens / micro-batch / rank: 32 x 512 = 16,384
Tokens / micro-batch: 16,384
Total batch size 524,288 => gradient accumulation steps: 32
Scaling weight decay from 0.200000 to 0.800000 for depth 6
GPT(
  (transformer): ModuleDict(
    (wte): Embedding(32768, 64)
    (h): ModuleList(
      (0): Block(
        (attn): CausalSelfAttention(
          (c_q): Linear(in_features=64, out_features=64, bias=False)
          (c_k): Linear(in_features=64, out_features=64, bias=False)
          (c_v): Linear(in_features=64, out_features=64, bias=False)
          (c_proj): Linear(in_features=64, out_features=64, bias=False)
        )
        (mlp): MLP(
          (c_fc): Linear(in_features=64, out_features=256, bias=False)
          (c_proj): Linear(in_features=256, out_features=64, bias=False)
        )
      )
      (1): Block(
        (attn): CausalSelfAttention(
          (c_q): Linear(in_features=64, out_features=64, bias=False)
          (c_k): Linear(in_features=64, out_features=64, bias=False)
          (c_v): Linear(in_features=64, out_features=64, bias=False)
          (c_proj): Linear(in_features=64, out_features=64, bias=False)
          (ve_gate): Linear(in_features=32, out_features=1, bias=False)
        )
        (mlp): MLP(
          (c_fc): Linear(in_features=64, out_features=256, bias=False)
          (c_proj): Linear(in_features=256, out_features=64, bias=False)
        )
      )
      (2): Block(
        (attn): CausalSelfAttention(
          (c_q): Linear(in_features=64, out_features=64, bias=False)
          (c_k): Linear(in_features=64, out_features=64, bias=False)
          (c_v): Linear(in_features=64, out_features=64, bias=False)
          (c_proj): Linear(in_features=64, out_features=64, bias=False)
        )
        (mlp): MLP(
          (c_fc): Linear(in_features=64, out_features=256, bias=False)
          (c_proj): Linear(in_features=256, out_features=64, bias=False)
        )
      )
      (3): Block(
        (attn): CausalSelfAttention(
          (c_q): Linear(in_features=64, out_features=64, bias=False)
          (c_k): Linear(in_features=64, out_features=64, bias=False)
          (c_v): Linear(in_features=64, out_features=64, bias=False)
          (c_proj): Linear(in_features=64, out_features=64, bias=False)
          (ve_gate): Linear(in_features=32, out_features=1, bias=False)
        )
        (mlp): MLP(
          (c_fc): Linear(in_features=64, out_features=256, bias=False)
          (c_proj): Linear(in_features=256, out_features=64, bias=False)
        )
      )
      (4): Block(
        (attn): CausalSelfAttention(
          (c_q): Linear(in_features=64, out_features=64, bias=False)
          (c_k): Linear(in_features=64, out_features=64, bias=False)
          (c_v): Linear(in_features=64, out_features=64, bias=False)
          (c_proj): Linear(in_features=64, out_features=64, bias=False)
        )
        (mlp): MLP(
          (c_fc): Linear(in_features=64, out_features=256, bias=False)
          (c_proj): Linear(in_features=256, out_features=64, bias=False)
        )
      )
      (5): Block(
        (attn): CausalSelfAttention(
          (c_q): Linear(in_features=64, out_features=64, bias=False)
          (c_k): Linear(in_features=64, out_features=64, bias=False)
          (c_v): Linear(in_features=64, out_features=64, bias=False)
          (c_proj): Linear(in_features=64, out_features=64, bias=False)
          (ve_gate): Linear(in_features=32, out_features=1, bias=False)
        )
        (mlp): MLP(
          (c_fc): Linear(in_features=64, out_features=256, bias=False)
          (c_proj): Linear(in_features=256, out_features=64, bias=False)
        )
      )
    )
  )
  (lm_head): Linear(in_features=64, out_features=32768, bias=False)
  (value_embeds): ModuleDict(
    (1): Embedding(32768, 64)
    (3): Embedding(32768, 64)
    (5): Embedding(32768, 64)
  )
)